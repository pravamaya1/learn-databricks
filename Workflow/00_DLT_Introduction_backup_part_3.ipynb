{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ae2a7ea-e5fd-4427-b374-85f100e641c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DLT works with three types of Datasets\n",
    "# 1. Streaming tables(Permanent/ temporary) - Uswd as append Data Scources, Incremental data\n",
    "#2.Materialized view - Used for transfermations, aggregation, aggregations or computations\n",
    "#3.Views - Used for intermidiate Transformations, Not stored in Target Schema\n",
    "\n",
    "import dlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da5e1297-af60-46e2-aa0f-e32ccad867cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Part3 of DLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "904de8e5-5f6b-41e5-9a36-3b4deb356994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_order_status = spark.conf.get(\"custom.orderStatus\",\"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "181ec5ac-5f78-4b4a-9f86-05ad36fec416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Auto Loader in DLT\n",
    "# Append Flow to union streaming Tables\n",
    "# Pass parameters in DLT pipeline\n",
    "# Generate DLT tables dynamically\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "550efbe5-1716-4259-9adc-a435ad6eddc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Streaming Table for Orders\n",
    "@dlt.table(\n",
    "    table_properties={\"quality\": \"bronze\"},\n",
    "    comment = \"Orders Bronze Table\"\n",
    ")\n",
    "def orders_bronze():\n",
    "    df= spark.readStream.table(\"dev.bronze.orders_raw\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30063783-06d8-4f71-86fe-6d9a07cd97e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a Streaming Table for Autoloader\n",
    "@dlt.table(\n",
    "    table_properties={\"quality\": \"bronze\"},\n",
    "    comment=\"order Autoloader Bronze Table\",\n",
    "    name=\"orders_autoloader_bronze\"\n",
    ")\n",
    "def func_autoloader():\n",
    "    df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.schemaHints\", \"o_orderkey long, o_custkey long, o_orderstatus string, o_totalprice decimal(18,2), o_orderdate date, o_orderpriority string, o_clerk string, o_shippriority integer, o_comment string\")\n",
    "    .option(\"cloudFiles.schemaLocation\",\"/Volumes/dev/etl/landing/autoloader/schemas/1/\")\n",
    "    .option(\"cloudFiles.format\", \"CSV\")\n",
    "    .option(\"pathGlobfilter\", \"*.csv\")\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"none\")\n",
    "    .load('/Volumes/dev/etl/landing/files/')\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32bc72b2-6754-4cfe-8dcb-214595bf5f07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If two streaming tables are joined together by Union, then all the data will read everytime. So avoid this and data should be read incrementally we have use appendFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa1defe0-9fe8-462d-b971-59b4afa450fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Union of Two streaming tables order_bronze and orders_autoloader_bronze\n",
    "\n",
    "dlt.create_streaming_table(\"orders_union_bronze\")\n",
    "\n",
    "#AppendFlow\n",
    "\n",
    "@dlt.append_flow(\n",
    "    target = \"orders_union_bronze\"\n",
    "    )\n",
    "\n",
    "def order_delta_append():\n",
    "    df = spark.readStream.table(\"LIVE.orders_bronze\")\n",
    "    return df\n",
    "    \n",
    "@dlt.append_flow(\n",
    "    target = \"orders_union_bronze\"\n",
    "    )\n",
    "\n",
    "def order_autoloader_append():\n",
    "    df = spark.readStream.table(\"LIVE.orders_autoloader_bronze\")\n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "755bfec4-a511-43e1-82bd-6fe5bfb34c62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a Materialized View for CUstomers\n",
    "\n",
    "@dlt.table(\n",
    "    table_properties={\"quality\": \"bronze\"},\n",
    "    comment = \"Customer Bronze Table\",\n",
    "    name = \"customers_bronze\"\n",
    ")\n",
    "def cust_bronze():\n",
    "    df = spark.read.table(\"dev.bronze.customers_raw\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d085f344-dd33-4fdd-8580-fc8b7d1c04d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#CREATE a view to join orders with customers\n",
    "@dlt.table(\n",
    "    comment = \"joined_view\",\n",
    "    \n",
    ")\n",
    "def joined_vw():\n",
    "    df_c= spark.read.table(\"LIVE.customers_bronze\")\n",
    "    df_o= spark.read.table(\"LIVE.orders_union_bronze\")\n",
    "    df_join = df_o.join(df_c, on=df_c.c_custkey == df_o.o_custkey, how =\"left_outer\" )\n",
    "    return df_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f618f47-f0e3-4800-98be-2af8dba57405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CREATE MV to add a new column\n",
    "# Change the name of silver table from \"joined_silver\" to \"order_silver\"\n",
    "from pyspark.sql.functions import current_timestamp,sum,count\n",
    "@dlt.table(\n",
    "    table_properties= {\"quality\": \"silver\"},\n",
    "    comment = \"Joined_table\",\n",
    "    name = \"orders_silver\"\n",
    ")\n",
    "def joined_silver():\n",
    "    df= spark.read.table(\"LIVE.joined_vw\").withColumn(\"_insert_date\", current_timestamp())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c058a13-52f9-4d62-b51d-402189a9b2fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Aggregate based on c_mktsegment and find the count of order(c_orderkey)\n",
    "#Added a new column and modified the a column from \"sum_orders\" to \"count_orders\"\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "for _status in _order_status.split(\",\"):\n",
    "\n",
    "    @dlt.table(\n",
    "        table_properties={\"quality\": \"gold\"},\n",
    "        comment = \"Orders aggregate table\",\n",
    "        name = f\"order_agg_{_status}_gold\"\n",
    "        )\n",
    "    def func():\n",
    "        df= spark.read.table(\"LIVE.orders_silver\")\n",
    "        df_final= df.where(f\"o_orderstatus = '{_status}'\").groupBy(\"c_mktsegment\").agg(count(\"o_orderkey\").alias (\"count_orders\"),sum(\"o_totalprice\").alias(\"sum_totalprice\")).withColumn(\"_insert_date\", current_timestamp())\n",
    "        return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a11165d1-5f5d-4328-82e1-c826482d1382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_DLT_Introduction_backup_part_3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
